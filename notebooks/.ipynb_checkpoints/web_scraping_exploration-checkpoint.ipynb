{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = 'https://www.indeed.com/jobs?q=data+science&l=Denver%2C+CO#&start=10'\n",
    "\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text, features='html.parser')\n",
    "\n",
    "# def extract_company_from_result(soup): \n",
    "#   companies = []\n",
    "#   for div in soup.find_all(name=”div”, attrs={“class”:”row”}):\n",
    "#     company = div.find_all(name=”span”, attrs={“class”:”company”})\n",
    "#     if len(company) > 0:\n",
    "#       for b in company:\n",
    "#         companies.append(b.text.strip())\n",
    "#     else:\n",
    "#       sec_try = div.find_all(name=”span”, attrs={“class”:”result-link-source”})\n",
    "#         for span in sec_try:\n",
    "#           companies.append(span.text.strip())\n",
    "#  return(companies)\n",
    " \n",
    "df = pd.DataFrame()\n",
    "\n",
    "def extract_company_from_result(soup):\n",
    "    companies = []\n",
    "    for div in soup.find_all(name='div', attrs={'class': 'row'}):\n",
    "        company = div.find_all(name='span', attrs={'class': 'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name='span', attrs={'class': 'result-link-source'})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return companies\n",
    "\n",
    "\n",
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "            jobs.append(a['title'])\n",
    "    return jobs\n",
    "\n",
    "# def extract_title_company(soup):\n",
    "#     companies = []\n",
    "#     jobs = []\n",
    "#     for div in soup.find_all(name='div', attrs={'class': 'row'}):\n",
    "#         company = div.find_all(name='span', attrs={'class': 'company'})\n",
    "#         if len(company) > 0:\n",
    "#             for b in company:\n",
    "#                 companies.append(b.text.strip())\n",
    "#     for div in soup.find_all(name='div', attrs={'class': 'row'}):\n",
    "#         for a in div.find_all(name='a', attrs={'data-tn-element': 'jobTitle'}):\n",
    "#             jobs.append(a['title'])\n",
    "#     return companies, jobs\n",
    "\n",
    "    \n",
    "def extract_locations_from_results(soup):\n",
    "    locations = []\n",
    "    for div in soup.find_all('div', attrs={'class': 'recJobLoc'}):\n",
    "        loc = div['data-rc-loc']\n",
    "        locations.append(loc)\n",
    "    return locations\n",
    "\n",
    "def extract_title_company_location(soup):\n",
    "    jobs = extract_job_title_from_result(soup)\n",
    "    companies = extract_company_from_result(soup)\n",
    "    locations = extract_locations_from_results(soup)\n",
    "    return jobs, companies, locations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Companies</th>\n",
       "      <th>Locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Science Modeler</td>\n",
       "      <td>Avero</td>\n",
       "      <td>Boulder, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Jr. Big Data Engineer</td>\n",
       "      <td>Enhance IT</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Aegis Premier Technologies</td>\n",
       "      <td>Westminster, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Willis Towers Watson</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Valen Analytics</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sr Business Analyst / Data Scientist</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Valen Analytics</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Science Analyst</td>\n",
       "      <td>Payfone</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Pricesenz</td>\n",
       "      <td>Broomfield, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>Maxar Technologies</td>\n",
       "      <td>Westminster, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Data Science Journalist</td>\n",
       "      <td>Alteryx, Inc.</td>\n",
       "      <td>Broomfield, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>Arrow Electronics, Inc.</td>\n",
       "      <td>Centennial, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Head of Data Science</td>\n",
       "      <td>Frontdoor</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Dataiku</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Nexthealth</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>The18</td>\n",
       "      <td>Boulder, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Data Warehouse Engineer</td>\n",
       "      <td>Seen by Indeed</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seen by Indeed</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Jobs                   Companies  \\\n",
       "0                   Data Science Modeler                       Avero   \n",
       "1                  Jr. Big Data Engineer                  Enhance IT   \n",
       "2                         Data Scientist  Aegis Premier Technologies   \n",
       "3                         Data Scientist        Willis Towers Watson   \n",
       "4                         Data Scientist             Valen Analytics   \n",
       "5   Sr Business Analyst / Data Scientist                     PHOENIX   \n",
       "6                          Data Engineer             Valen Analytics   \n",
       "7                   Data Science Analyst                     Payfone   \n",
       "8                         Data Scientist                   Pricesenz   \n",
       "9                    Data Science Intern          Maxar Technologies   \n",
       "10               Data Science Journalist               Alteryx, Inc.   \n",
       "11                   Data Science Intern     Arrow Electronics, Inc.   \n",
       "12                        Data Scientist                    Deloitte   \n",
       "13                  Head of Data Science                   Frontdoor   \n",
       "14                        Data Scientist                     Dataiku   \n",
       "15                 Senior Data Scientist                  Nexthealth   \n",
       "16                        Data Scientist                       The18   \n",
       "17               Data Warehouse Engineer              Seen by Indeed   \n",
       "18                         Data Engineer              Seen by Indeed   \n",
       "\n",
       "          Locations  \n",
       "0       Boulder, CO  \n",
       "1        Denver, CO  \n",
       "2   Westminster, CO  \n",
       "3        Denver, CO  \n",
       "4        Denver, CO  \n",
       "5        Denver, CO  \n",
       "6        Denver, CO  \n",
       "7        Denver, CO  \n",
       "8    Broomfield, CO  \n",
       "9   Westminster, CO  \n",
       "10   Broomfield, CO  \n",
       "11   Centennial, CO  \n",
       "12       Denver, CO  \n",
       "13       Denver, CO  \n",
       "14       Denver, CO  \n",
       "15       Denver, CO  \n",
       "16      Boulder, CO  \n",
       "17       Denver, CO  \n",
       "18       Denver, CO  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs, companies, locations = extract_title_company_location(soup)\n",
    "df = pd.DataFrame()\n",
    "df['Jobs'] = jobs\n",
    "df['Companies'] = companies\n",
    "df['Locations'] = locations\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp, job in zip(companies, jobs):\n",
    "    print(f'job: {job}')\n",
    "    print(f'company: {comp}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Companies'] = companies\n",
    "df['Jobs'] = jobs\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_url = 'https://www.indeed.com/jobs?q=data+science&l=Denver%2C+CO&limit=50&radius=25'\n",
    "cont_url = 'https://www.indeed.com/jobs?q=data+science&l=Denver%2C+CO&limit=50&radius=25&start='\n",
    "start = [num for num in range(50, 251, 50)]\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [starting_url]\n",
    "for i in start:\n",
    "    base_url = starting_url\n",
    "    addition = f'&start={i}'\n",
    "    base_url += addition\n",
    "    url_list.append(base_url)\n",
    "    \n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies, jobs = [], []\n",
    "for url in url_list:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, features='html.parser')\n",
    "    companies_temp, job_temp = extract_title_company(soup)\n",
    "    companies.extend(companies_temp)\n",
    "    jobs.extend(job_temp)\n",
    "    time.sleep(5)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Jobs'] = jobs\n",
    "df['Companies'] = companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_company(soup):\n",
    "    companies = []\n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={'class': 'row'}):\n",
    "        company = div.find(name='span', attrs={'class': 'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "    for div in soup.find_all(name='div', attrs={'class': 'row'}):\n",
    "        for a in div.find_all(name='a', attrs={'data-tn-element': 'jobTitle'}):\n",
    "            jobs.append(a['title'])\n",
    "    return companies, jobs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_title_company(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.indeed.com/jobs?q=data+science&l=Denver%2C+CO&limit=50&radius=25'\n",
    "\n",
    "page = requests.get(url2)\n",
    "soup = BeautifulSoup(page.text, features='lxml')\n",
    "\n",
    "soup.find_all('div', attrs={'class': 'jobsearch-SerpJobCard unifiedRow row Result clickcard'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in soup.find_all('div', attrs={'class': 'sjcl'}):\n",
    "    print(soup.find('div', attrs={'class': 'recJobLoc'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in soup.find_all('div', attrs={'class': 'recJobLoc'}):\n",
    "    print(div['data-rc-loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/josh-mantovani/Desktop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_company_from_result(soup):\n",
    "    companies = []\n",
    "    for div in soup.find_all(name='div', attrs={'class': 'row'}):\n",
    "        company = div.find_all(name='span', attrs={'class': 'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name='span', attrs={'class': 'result-link-source'})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return companies\n",
    "\n",
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "            jobs.append(a['title'])\n",
    "    return jobs\n",
    "\n",
    "def extract_locations_from_results(soup):\n",
    "    locations = []\n",
    "    for div in soup.find_all('div', attrs={'class': 'recJobLoc'}):\n",
    "        loc = div['data-rc-loc']\n",
    "        locations.append(loc)\n",
    "    return locations\n",
    "\n",
    "def extract_comprating_from_results(soup):\n",
    "    ratings = []\n",
    "    for div in soup.find_all('div', attrs={'class': 'sjcl'}):\n",
    "        span = div.find('span', attrs={'class': 'ratingsContent'})\n",
    "        if span:\n",
    "            rating = span.text.strip()\n",
    "            ratings.append(float(rating))\n",
    "        else:\n",
    "            ratings.append(None)\n",
    "    return ratings\n",
    "\n",
    "def extract_easyapply_from_results(soup):\n",
    "    easy_apply = []\n",
    "    for div in soup.find_all('div', attrs={'class': 'row'}):\n",
    "        if div.find('span', attrs={'class': 'iaLabel'}):\n",
    "            easy_apply.append('Easy Apply')\n",
    "        elif div.find('span', attrs={'class': 'iaLabel'}) == None:\n",
    "            easy_apply.append('Not Easy Apply')\n",
    "    return easy_apply\n",
    "\n",
    "def extract_title_company_location_ea(soup):\n",
    "    jobs = extract_job_title_from_result(soup)\n",
    "    companies = extract_company_from_result(soup)\n",
    "    locations = extract_locations_from_results(soup)\n",
    "    easy_apply = extract_easyapply_from_results(soup)\n",
    "    ratings = extract_comprating_from_results(soup)\n",
    "    return jobs, companies, locations, easy_apply, ratings\n",
    "\n",
    "def get_last_page(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.text, features='lxml')\n",
    "    pn_list = []\n",
    "    for pn in soup.find_all('span', attrs={'class': 'pn'}):\n",
    "        page_number = list(pn.text)\n",
    "        pn_list.append(page_number)\n",
    "    if len(pn_list[-2]) > 1:\n",
    "        last_page = int(pn_list[-2][0] + pn_list[-2][1])\n",
    "    else:\n",
    "        last_page = int(pn_list[-2][0])\n",
    "    return last_page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6, None, 4.0, None, None, None, None, 3.7, None, None, 3.5, 4.0, 4.1, 3.7, 4.6, 3.7, None, None, 4.0, None, 3.0, 4.3, None, 4.0, 4.0, 3.2, None, None, 3.9, None, None, None, 3.7, None, 4.5, 4.0, None, 4.2, None, 3.9, None, 3.8, 3.8, 3.5, 3.9, None, None, 4.1, None, 3.3, 4.6, 3.9, None, 3.5, None, None, 4.4, 3.6, 3.9]\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.indeed.com/jobs?q=data+science&l=Denver,+CO&limit=50&radius=25'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text, features='lxml')\n",
    "print(extract_comprating_from_results(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
